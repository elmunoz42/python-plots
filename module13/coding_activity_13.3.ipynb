{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "415b894be03fe9cf00d3d76fa3919486",
     "grade": false,
     "grade_id": "cell-c4ad417ef9c632ba",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Codio Activity 13.3: Determining the Logistic Functions Parameters\n",
    "\n",
    "**Expected Time = 60 minutes** \n",
    "\n",
    "**Total Points = 50** \n",
    "\n",
    "This activity focuses on determining the appropriate parameters for the logistic model using optimization.  To begin, you will write a function to represent $\\sigma(x)$.  Then, you will use the scikit-learn metric `log_loss` to evaluate the cross entropy based on the probabilities.  Finally, you will explore different parameters to select that which minimizes the cross entropy.\n",
    "\n",
    "#### Index\n",
    "\n",
    "- [Problem 1](#-Problem-1)\n",
    "- [Problem 2](#-Problem-2)\n",
    "- [Problem 3](#-Problem-3)\n",
    "- [Problem 4](#-Problem-4)\n",
    "- [Problem 5](#-Problem-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins = sns.load_dataset('penguins').dropna()\n",
    "penguins = penguins.loc[(penguins['species'] == 'Adelie') | (penguins['species'] == 'Gentoo')]\n",
    "X = penguins.drop('species', axis = 1)[['flipper_length_mm']]\n",
    "y = np.where(penguins.species == 'Adelie', 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Problem\n",
    "\n",
    "As discussed in the lectures, with gaussian assumptions of the data distributions, you are able to directly compute the parameters for the logistic model.  However, this is not going to work in higher dimensional settings.  As you see here, this assumption seems reasonable for the flipper length of our penguins data.\n",
    "\n",
    "<center>\n",
    "    <img src = 'images/flipperdist.png' />\n",
    "</center>\n",
    "\n",
    "But rather than using the mean and variance of these distributions learned through maximum likelihood, we will frame the problem of parameter estimation as one of minimization, and among a range of possible $\\beta$ values, select that which minimized cross entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2c750b94878fe1eb0c98c2a5aa59b9af",
     "grade": false,
     "grade_id": "cell-e1e88c4555a6b649",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#-Index)\n",
    "\n",
    "### Problem 1\n",
    "\n",
    "#### Sigma Function\n",
    "\n",
    "**10 Points**\n",
    "\n",
    "To begin, you are to define a sigmoid function with `sigma` below that takes in an array $X$, $\\beta_0$, $\\beta_1$, and returns \n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1x)}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ef5c600eaa0ba518e83c2824735d0817",
     "grade": false,
     "grade_id": "cell-533b33420380568c",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "def sigma(x, beta_0, beta_1):\n",
    "    \"\"\"\n",
    "    This function evaluates the sigmoid function with \n",
    "    given parameters beta_0 and beta_1\n",
    "    \n",
    "    Argments\n",
    "    --------\n",
    "    x: np.array\n",
    "       domain for evaluation of sigma\n",
    "    beta_0: float\n",
    "       intercept term of linear function\n",
    "    beta_1: float\n",
    "        slope of linear term\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    np.array \n",
    "    \"\"\"\n",
    "    return None\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Answer check\n",
    "print(sigma(X, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e9b773c99121536609909f406a079fc4",
     "grade": true,
     "grade_id": "cell-0175d9df63633523",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cef7507bb68c5245267e20b1cd25fecb",
     "grade": false,
     "grade_id": "cell-2dedf86070c9d2da",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#-Index)\n",
    "\n",
    "### Problem 2\n",
    "\n",
    "#### Using a given $\\beta_0$ and $\\beta_1$ for predictions\n",
    "\n",
    "**10 Points**\n",
    "\n",
    "Now, your `sigma` function should return probabilities for making a classification.  Use your sigma function and the given $\\beta_0$ and $\\beta_1$ below to create predictions on X.  If the returned value is $\\geq .5$ predict class 1 and otherwise predict 0. Assign the probabilites as an array to `probs` and the subsequent predictions based on these probabilities to `predictions`.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_0 = -165\n",
    "beta_1 = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "47d439046c87b0c9ede8027924069f9f",
     "grade": false,
     "grade_id": "cell-8086826f4d0f39db",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "probs = ''\n",
    "predictions = ''\n",
    "\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Answer check\n",
    "np.unique(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d2ef4b4008ce02552e2c1a2b66b99e11",
     "grade": true,
     "grade_id": "cell-0ddf5a5220d1c47e",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "008865c936b8ecc4c25b70c8f27a270f",
     "grade": false,
     "grade_id": "cell-6810d9f56a3486e3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#-Index)\n",
    "\n",
    "### Problem 3\n",
    "\n",
    "#### Computing the Cross Entropy\n",
    "\n",
    "**10 Points**\n",
    "\n",
    "Recall that our goal is to find the $\\beta_0$ and $\\beta_1$ values that minimize the cross entropy.  The cross entropy was given to us as\n",
    "\n",
    "$$-\\sum_{i = 1} ^ N \\big( (1 - y_i)\\log(1 - \\sigma(x_i)\\big) + \\big(y_i(\\log \\sigma(x_i) \\big)$$\n",
    "\n",
    "Scikitlearn has an implementation of this in the metrics module with the `log_loss` function (imported again below).  To use this, we want to pass an array of probabilities for both the positive and negative classes against the true $y$ values.  Below, this array is created for you using the earlier probabilities.  Compute the log loss of your predicted probabilities and assign the value to `loss1` below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_array = np.concatenate((probs.values, (1 - probs).values), axis = 1)\n",
    "pd.DataFrame(prob_array, columns = ['probability 0', 'probability 1']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7da78870f0121c19430ac1525fb37aac",
     "grade": false,
     "grade_id": "cell-4a2f889191ea3f75",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "loss1 = ''\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Answer check\n",
    "print(loss1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "682768392ca9286de1ed28b115d30eda",
     "grade": true,
     "grade_id": "cell-2152228bf0adfc25",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "15b47dfcd543679718fe5888199d85ac",
     "grade": false,
     "grade_id": "cell-2b7720b972f0e0e5",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#-Index)\n",
    "\n",
    "### Problem 4\n",
    "\n",
    "#### Comparing Loss across Parameters\n",
    "\n",
    "**10 Points**\n",
    "\n",
    "Remember that your goal is to identify the parameters that **minimize** the log loss.  However, we know the rough values of this from our last assignment -- at least the values that come from the solved optimization problem with scikitlearn.  \n",
    "\n",
    "Accordingly, below consider an array of parameters `beta_1s`.  Complete the code below to loop over these parameters, determine probabilities using them, create a probability array, and compute the log loss of these predictions.  \n",
    "\n",
    "Keep track of the losses as a list in `losses`.  \n",
    "\n",
    "Finally, determine the beta_1 that minimizes the log_loss and assign it as a float to `best_beta1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_1s = np.linspace(0, 1, 1000) #array of beta_1's to use\n",
    "beta_0 = -142 #beta_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "776805a38b99e099e425607c8bfac49d",
     "grade": false,
     "grade_id": "cell-4777ea1367c2910f",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "losses = []\n",
    "for beta_1 in beta_1s:\n",
    "    probs = ''\n",
    "    prob_array = ''\n",
    "    losses.append(None)\n",
    "    \n",
    "best_beta1 = ''\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Answer check\n",
    "print(best_beta1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3f835655f900ff640301ca711b48ee5a",
     "grade": true,
     "grade_id": "cell-e1daa05545cc653e",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "62378c4db15ac9f97756390ca83a603b",
     "grade": false,
     "grade_id": "cell-bf9a48b3efcd7682",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#-Index)\n",
    "\n",
    "### Problem 5\n",
    "\n",
    "#### Comparing the results to `LogisticRegression`\n",
    "\n",
    "**10 Points**\n",
    "\n",
    "Behind the scenes, this is what the `LogisticRegression` estimator is doing.  However, unlike our guess and test strategy above, a solver that implements gradient descent is used to target the optimal parameter values.  Specifically, the solvers `'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'` are available and `liblinear` is the default.  \n",
    "\n",
    "As a check, implement a logistic regression estimator using scikitlearn with all default settings and examine the coefficient.  Is it close to what you selected in problem 4 above?  \n",
    "\n",
    "Assign your fit estimator as `log_reg` the coefficient as a float to `coef` and the absolute difference between the coefficient from scikitlearn and that of `best_beta1` from problem 4 as difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "33328a77fc7a7758ac3ea06f0c3e73c8",
     "grade": false,
     "grade_id": "cell-39462d43514bd0d8",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "logreg = ''\n",
    "coef = ''\n",
    "difference = ''\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Answer check\n",
    "print(f'Beta_1 from sklearn: {coef}\\nBeta_1 from our minimization: {best_beta1}\\nDifference: {difference: .2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0a0013335ff4ded050137919f7b083e6",
     "grade": true,
     "grade_id": "cell-3391b8637674a4a9",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
