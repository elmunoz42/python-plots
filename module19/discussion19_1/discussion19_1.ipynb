{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required Discussion 19:1: Building a Recommender System with SURPRISE\n",
    "\n",
    "This discussion focuses on exploring additional algorithms with the `Suprise` library to generate recommendations.  Your goal is to identify the optimal algorithm by minimizing the mean squared error using cross validation. You are also going to select a dataset to use from [grouplens](https://grouplens.org/datasets/movielens/) example datasets.  \n",
    "\n",
    "To begin, head over to [grouplens](https://grouplens.org/datasets/movielens/) and examine the different datasets available.  Choose one so that it is easy to create the data as expected in `Surprise` with user, item, and rating information.  Then, compare the performance of at least the `KNNBasic`, `SVD`, `NMF`, `SlopeOne`, and `CoClustering` algorithms to build your recommendations.  For more information on the algorithms see the documentation for the algorithm package [here](https://surprise.readthedocs.io/en/stable/prediction_algorithms_package.html).\n",
    "\n",
    "Share the results of your investigation and include the results of your cross validation and a basic description of your dataset with your peers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install SURPRISE (run this only once)\n",
    "# !pip install scikit-surprise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from surprise import Dataset, Reader, SVD, NMF, KNNBasic, SlopeOne, CoClustering\n",
    "from surprise.model_selection import cross_validate, train_test_split, GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          userId  movieId  rating   timestamp\n",
      "10685861   66954      781     5.0   850944577\n",
      "1552723     9877      574     4.0   945495614\n",
      "6145184    38348     1088     2.0   999974867\n",
      "16268584  101952     2706     1.0  1203077565\n",
      "22418634  140400   275079     3.5  1653782463\n"
     ]
    }
   ],
   "source": [
    "# Load the ratings.csv file\n",
    "ratings_full_df = pd.read_csv('data/ratings.csv')\n",
    "\n",
    "# Sample a smaller subset (e.g., 10% of ratings)\n",
    "ratings_df = ratings_full_df.sample(frac=0.1, random_state=42)\n",
    "\n",
    "# Display the first few rows to verify the data\n",
    "print(ratings_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ratings summary:\n",
      "count    3.200020e+06\n",
      "mean     3.540462e+00\n",
      "std      1.058937e+00\n",
      "min      5.000000e-01\n",
      "25%      3.000000e+00\n",
      "50%      3.500000e+00\n",
      "75%      4.000000e+00\n",
      "max      5.000000e+00\n",
      "Name: rating, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Summary statistics for the ratings\n",
    "print(\"\\nRatings summary:\")\n",
    "print(ratings_df['rating'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset has 197270 users, 42809 movies, and 3200020 ratings\n"
     ]
    }
   ],
   "source": [
    "# Count unique users and movies\n",
    "n_users = ratings_df['userId'].nunique()\n",
    "n_movies = ratings_df['movieId'].nunique()\n",
    "n_ratings = len(ratings_df)\n",
    "print(f\"\\nDataset has {n_users} users, {n_movies} movies, and {n_ratings} ratings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Reader object\n",
    "reader = Reader(rating_scale=(0.5, 5.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating SVD...\n",
      "Evaluating RMSE, MAE of algorithm SVD on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "RMSE (testset)    0.8956  0.8963  0.8965  0.8966  0.8973  0.8965  0.0005  \n",
      "MAE (testset)     0.6846  0.6845  0.6853  0.6855  0.6857  0.6851  0.0005  \n",
      "Fit time          41.77   42.60   40.54   44.60   40.41   41.98   1.54    \n",
      "Test time         5.09    6.02    6.73    4.97    6.09    5.78    0.66    \n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'mean'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 26\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEvaluating \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m cross_validate(algorithm, data, measures\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRMSE\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMAE\u001b[39m\u001b[38;5;124m'\u001b[39m], cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     23\u001b[0m results[name] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_rmse\u001b[39m\u001b[38;5;124m'\u001b[39m: cv_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_rmse\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmean(),\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_mae\u001b[39m\u001b[38;5;124m'\u001b[39m: cv_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_mae\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmean(),\n\u001b[0;32m---> 26\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfit_time\u001b[39m\u001b[38;5;124m'\u001b[39m: cv_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfit_time\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmean(),\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_time\u001b[39m\u001b[38;5;124m'\u001b[39m: cv_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_time\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m     28\u001b[0m }\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'mean'"
     ]
    }
   ],
   "source": [
    "# Load the data into SURPRISE format\n",
    "data = Dataset.load_from_df(ratings_df[['userId', 'movieId', 'rating']], reader)\n",
    "\n",
    "# Define the algorithms to compare ( I had to do one at a time due to my laptop's performance issues...)\n",
    "algorithms = {\n",
    "    'SVD': SVD(),\n",
    "    # 'NMF': NMF(),\n",
    "    # 'KNNBasic': KNNBasic(),\n",
    "    # 'SlopeOne': SlopeOne(),\n",
    "    # 'CoClustering': CoClustering()\n",
    "}\n",
    "\n",
    "# Dictionary to store the results\n",
    "results = {}\n",
    "\n",
    "# Perform cross-validation for each algorithm\n",
    "for name, algorithm in algorithms.items():\n",
    "    print(f\"\\nEvaluating {name}...\")\n",
    "    cv_results = cross_validate(algorithm, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)\n",
    "    results[name] = {\n",
    "        'test_rmse': cv_results['test_rmse'].mean(),\n",
    "        'test_mae': cv_results['test_mae'].mean(),\n",
    "        'fit_time': cv_results['fit_time'].mean(),\n",
    "        'test_time': cv_results['test_time'].mean()\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVD Algorithm Results\r\n",
    "\r\n",
    "| Metric | Fold 1 | Fold 2 | Fold 3 | Fold 4 | Fold 5 | Mean | Std |\r\n",
    "|--------|--------|--------|--------|--------|--------|------|------|\r\n",
    "| RMSE (testset) | 0.8956 | 0.8963 | 0.8965 | 0.8966 | 0.8973 | 0.8965 | 0.0005 |\r\n",
    "| MAE (testset) | 0.6846 | 0.6845 | 0.6853 | 0.6855 | 0.6857 | 0.6851 | 0.0005 |\r\n",
    "| Fit time | 41.77 | 42.60 | 40.54 | 44.60 | 40.41 | 41.98 | 1.54 |\r\n",
    "| Test time | 5.09 | 6.02 | 6.73 | 4.97 | 6.09 | 5.78 | 0.66 |\n",
    "\n",
    "These are solid results for SVD. An RMSE of about 0.897 means your predictions are, on average, within about 0.9 stars of the actual ratings users gave. The consistency across folds (low standard deviation of 0.001) suggests the model is stable. 0.12 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating NMF...\n",
      "Evaluating RMSE, MAE of algorithm NMF on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "RMSE (testset)    0.9430  0.9459  0.9428  0.9461  0.9472  0.9450  0.0018  \n",
      "MAE (testset)     0.7173  0.7187  0.7170  0.7188  0.7199  0.7183  0.0011  \n",
      "Fit time          58.66   58.54   61.70   75.01   64.71   63.72   6.08    \n",
      "Test time         3.88    3.57    4.45    4.37    3.72    4.00    0.35    \n",
      "Type of cv_results: <class 'dict'>\n",
      "Keys in cv_results: dict_keys(['test_rmse', 'test_mae', 'fit_time', 'test_time'])\n",
      "Key: test_rmse, Type: <class 'numpy.ndarray'>\n",
      "Mean test_rmse: 0.9450117363388463\n",
      "Key: test_mae, Type: <class 'numpy.ndarray'>\n",
      "Mean test_mae: 0.7183499573462457\n",
      "Key: fit_time, Type: <class 'tuple'>\n",
      "Value fit_time: (58.65761923789978, 58.538286447525024, 61.69825291633606, 75.01252555847168, 64.7076563835144)\n",
      "Key: test_time, Type: <class 'tuple'>\n",
      "Value test_time: (3.8787639141082764, 3.565955877304077, 4.453284978866577, 4.3660924434661865, 3.720841407775879)\n"
     ]
    }
   ],
   "source": [
    "# Perform cross-validation for NMF algorithm\n",
    "name = 'NMF'\n",
    "algorithm = NMF()\n",
    "print(f\"\\nEvaluating {name}...\")\n",
    "cv_results = cross_validate(algorithm, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)\n",
    "\n",
    "# Let's check the type and structure of cv_results\n",
    "print(\"Type of cv_results:\", type(cv_results))\n",
    "print(\"Keys in cv_results:\", cv_results.keys())\n",
    "\n",
    "# Try accessing the results differently\n",
    "for key in cv_results:\n",
    "    print(f\"Key: {key}, Type: {type(cv_results[key])}\")\n",
    "    if hasattr(cv_results[key], 'mean'):\n",
    "        print(f\"Mean {key}:\", cv_results[key].mean())\n",
    "    else:\n",
    "        print(f\"Value {key}:\", cv_results[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NMF Algorithm Results\n",
    "\n",
    "| Metric | Fold 1 | Fold 2 | Fold 3 | Fold 4 | Fold 5 | Mean | Std |\n",
    "|--------|--------|--------|--------|--------|--------|------|------|\n",
    "| RMSE (testset) | 0.9430 | 0.9459 | 0.9428 | 0.9461 | 0.9472 | 0.9450 | 0.0018 |\n",
    "| MAE (testset) | 0.7173 | 0.7187 | 0.7170 | 0.7188 | 0.7199 | 0.7183 | 0.0011 |\n",
    "| Fit time | 58.66 | 58.54 | 61.70 | 75.01 | 64.71 | 63.72 | 6.08 |\n",
    "| Test time | 3.88 | 3.57 | 4.45 | 4.37 | 3.72 | 4.00 | 0.35 |\n",
    "\n",
    "We got a better result with NMF than SVD but it took longer to fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating KNNBasic...\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Evaluating RMSE, MAE of algorithm KNNBasic on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "RMSE (testset)    1.0581  1.0530  1.0681  1.0645  1.0564  1.0600  0.0055  \n",
      "MAE (testset)     0.8375  0.8362  0.8495  0.8379  0.8381  0.8398  0.0049  \n",
      "Fit time          0.48    0.34    0.22    0.19    0.19    0.28    0.11    \n",
      "Test time         0.04    0.02    0.03    0.03    0.03    0.03    0.01    \n",
      "Type of cv_results: <class 'dict'>\n",
      "Keys in cv_results: dict_keys(['test_rmse', 'test_mae', 'fit_time', 'test_time'])\n",
      "Key: test_rmse, Type: <class 'numpy.ndarray'>\n",
      "Mean test_rmse: 1.0600159785897483\n",
      "Key: test_mae, Type: <class 'numpy.ndarray'>\n",
      "Mean test_mae: 0.8398237994384766\n",
      "Key: fit_time, Type: <class 'tuple'>\n",
      "Value fit_time: (0.4812617301940918, 0.3391084671020508, 0.21862483024597168, 0.19118523597717285, 0.1920008659362793)\n",
      "Key: test_time, Type: <class 'tuple'>\n",
      "Value test_time: (0.03916525840759277, 0.023507356643676758, 0.025650501251220703, 0.025326251983642578, 0.02572917938232422)\n"
     ]
    }
   ],
   "source": [
    "# Perform cross-validation for NMF algorithm\n",
    "name = 'KNNBasic'\n",
    "algorithm = KNNBasic(k=5, min_k=1, sim_options={'user_based': False})\n",
    "print(f\"\\nEvaluating {name}...\")\n",
    "# due to performance constraints I had to take a smaller sample of the data\n",
    "ratings_df_small = ratings_df.sample(frac=0.01, random_state=42)\n",
    "data_small = Dataset.load_from_df(ratings_df_small[['userId', 'movieId', 'rating']], reader)\n",
    "\n",
    "cv_results = cross_validate(algorithm, data_small, measures=['RMSE', 'MAE'], cv=5, verbose=True)\n",
    "\n",
    "# Let's check the type and structure of cv_results\n",
    "print(\"Type of cv_results:\", type(cv_results))\n",
    "print(\"Keys in cv_results:\", cv_results.keys())\n",
    "\n",
    "# Try accessing the results differently\n",
    "for key in cv_results:\n",
    "    print(f\"Key: {key}, Type: {type(cv_results[key])}\")\n",
    "    if hasattr(cv_results[key], 'mean'):\n",
    "        print(f\"Mean {key}:\", cv_results[key].mean())\n",
    "    else:\n",
    "        print(f\"Value {key}:\", cv_results[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN Algorithm Results\r\n",
    "\r\n",
    "| Metric | Fold 1 | Fold 2 | Fold 3 | Fold 4 | Fold 5 | Mean | Std |\r\n",
    "|--------|--------|--------|--------|--------|--------|------|------|\r\n",
    "| RMSE (testset) | 1.0581 | 1.0530 | 1.0681 | 1.0645 | 1.0564 | 1.0600 | 0.0055 |\r\n",
    "| MAE (testset) | 0.8375 | 0.8362 | 0.8495 | 0.8379 | 0.8381 | 0.8398 | 0.0049 |\r\n",
    "| Fit time | 0.48 | 0.34 | 0.22 | 0.19 | 0.19 | 0.28 | 0.11 |\r\n",
    "| Test time | 0.04 | 0.02 | 0.03 | 0.03 | 0.03 | 0.03 |\n",
    "\n",
    "KNN with a 5 fold gets a decent result MAE 0.8398, but is very resource intensive I had to reduce the sample size by 100X for it to work with my laptop. 0.01 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating SlopeOne...\n",
      "Evaluating RMSE, MAE of algorithm SlopeOne on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "RMSE (testset)    1.1313  1.1220  1.1010  1.1111  1.1292  1.1189  0.0114  \n",
      "MAE (testset)     0.8839  0.8727  0.8578  0.8630  0.8789  0.8713  0.0097  \n",
      "Fit time          0.74    0.73    0.55    0.56    0.59    0.63    0.08    \n",
      "Test time         0.03    0.04    0.02    0.02    0.02    0.03    0.01    \n",
      "Type of cv_results: <class 'dict'>\n",
      "Keys in cv_results: dict_keys(['test_rmse', 'test_mae', 'fit_time', 'test_time'])\n",
      "Key: test_rmse, Type: <class 'numpy.ndarray'>\n",
      "Mean test_rmse: 1.1188960572588067\n",
      "Key: test_mae, Type: <class 'numpy.ndarray'>\n",
      "Mean test_mae: 0.8712760526240453\n",
      "Key: fit_time, Type: <class 'tuple'>\n",
      "Value fit_time: (0.7404730319976807, 0.7308573722839355, 0.548473596572876, 0.5568242073059082, 0.5934646129608154)\n",
      "Key: test_time, Type: <class 'tuple'>\n",
      "Value test_time: (0.027343273162841797, 0.037239789962768555, 0.019870758056640625, 0.0216677188873291, 0.020784378051757812)\n"
     ]
    }
   ],
   "source": [
    "# Perform cross-validation for NMF algorithm\n",
    "name = 'SlopeOne'\n",
    "algorithm = SlopeOne()\n",
    "print(f\"\\nEvaluating {name}...\")\n",
    "# due to performance constraints I had to take a smaller sample of the data\n",
    "ratings_df_small = ratings_df.sample(frac=0.01, random_state=42)\n",
    "data_small = Dataset.load_from_df(ratings_df_small[['userId', 'movieId', 'rating']], reader)\n",
    "\n",
    "cv_results = cross_validate(algorithm, data_small, measures=['RMSE', 'MAE'], cv=5, verbose=True)\n",
    "\n",
    "# Let's check the type and structure of cv_results\n",
    "print(\"Type of cv_results:\", type(cv_results))\n",
    "print(\"Keys in cv_results:\", cv_results.keys())\n",
    "\n",
    "# Try accessing the results differently\n",
    "for key in cv_results:\n",
    "    print(f\"Key: {key}, Type: {type(cv_results[key])}\")\n",
    "    if hasattr(cv_results[key], 'mean'):\n",
    "        print(f\"Mean {key}:\", cv_results[key].mean())\n",
    "    else:\n",
    "        print(f\"Value {key}:\", cv_results[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SlopeOne Algorithm Results \r\n",
    "\r\n",
    "| Metric | Fold 1 | Fold 2 | Fold 3 | Fold 4 | Fold 5 | Mean | Std |\r\n",
    "|--------|--------|--------|--------|--------|--------|------|------|\r\n",
    "| RMSE (testset) | 1.1313 | 1.1220 | 1.1010 | 1.1111 | 1.1292 | 1.1189 | 0.0114 |\r\n",
    "| MAE (testset) | 0.8839 | 0.8727 | 0.8578 | 0.8630 | 0.8789 | 0.8713 | 0.0097 |\r\n",
    "| Fit time | 0.74 | 0.73 | 0.55 | 0.56 | 0.59 | 0.63 | 0.08 |\r\n",
    "| Test time | 0.03 | 0.04 | 0.02 | 0.02 | 0.02 | 0.03 | 0.01 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating CoClustering...\n",
      "Evaluating RMSE, MAE of algorithm CoClustering on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "RMSE (testset)    1.1320  1.1468  1.1499  1.1256  1.1517  1.1412  0.0104  \n",
      "MAE (testset)     0.8873  0.8986  0.9062  0.8847  0.9047  0.8963  0.0088  \n",
      "Fit time          1.27    1.23    1.28    1.13    1.30    1.24    0.06    \n",
      "Test time         0.01    0.01    0.07    0.01    0.07    0.04    0.03    \n",
      "Type of cv_results: <class 'dict'>\n",
      "Keys in cv_results: dict_keys(['test_rmse', 'test_mae', 'fit_time', 'test_time'])\n",
      "Key: test_rmse, Type: <class 'numpy.ndarray'>\n",
      "Mean test_rmse: 1.1411957991625283\n",
      "Key: test_mae, Type: <class 'numpy.ndarray'>\n",
      "Mean test_mae: 0.896310336858496\n",
      "Key: fit_time, Type: <class 'tuple'>\n",
      "Value fit_time: (1.270249843597412, 1.2317979335784912, 1.2778737545013428, 1.1315102577209473, 1.295992136001587)\n",
      "Key: test_time, Type: <class 'tuple'>\n",
      "Value test_time: (0.012739419937133789, 0.01221776008605957, 0.07467126846313477, 0.012514591217041016, 0.07095575332641602)\n"
     ]
    }
   ],
   "source": [
    "# Perform cross-validation for NMF algorithm\n",
    "name = 'CoClustering'\n",
    "algorithm = CoClustering()\n",
    "print(f\"\\nEvaluating {name}...\")\n",
    "# due to performance constraints I had to take a smaller sample of the data\n",
    "ratings_df_small = ratings_df.sample(frac=0.01, random_state=42)\n",
    "data_small = Dataset.load_from_df(ratings_df_small[['userId', 'movieId', 'rating']], reader)\n",
    "\n",
    "cv_results = cross_validate(algorithm, data_small, measures=['RMSE', 'MAE'], cv=5, verbose=True)\n",
    "\n",
    "# Let's check the type and structure of cv_results\n",
    "print(\"Type of cv_results:\", type(cv_results))\n",
    "print(\"Keys in cv_results:\", cv_results.keys())\n",
    "\n",
    "# Try accessing the results differently\n",
    "for key in cv_results:\n",
    "    print(f\"Key: {key}, Type: {type(cv_results[key])}\")\n",
    "    if hasattr(cv_results[key], 'mean'):\n",
    "        print(f\"Mean {key}:\", cv_results[key].mean())\n",
    "    else:\n",
    "        print(f\"Value {key}:\", cv_results[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MovieLens Recommendation Algorithms Comparison\n",
    "\n",
    "| Algorithm | RMSE | MAE | Fit Time (s) | Test Time (s) |\n",
    "|-----------|------|-----|--------------|--------------|\n",
    "| SVD | 0.8965 | 0.6851 | 41.98 | 5.78 |\n",
    "| NMF | 0.9450 | 0.7183 | 63.72 | 4.00 |\n",
    "| SlopeOne | 1.1189 | 0.8713 | 0.63 | 0.03 |\n",
    "| CoClustering | 1.1412 | 0.8963 | 1.24 | 0.04 |\n",
    "| KNNBasic | 1.06000 |0.83980 | 0.28 | 0.03 |\n",
    "\n",
    "## Performance Analysis\n",
    "\n",
    "1. **Best Accuracy**: SVD achieves the lowest RMSE (0.8965) and MAE (0.6851), making it the most accurate algorithm.\n",
    "\n",
    "2. **Speed-Accuracy Tradeoff**:\n",
    "   - SVD: Best accuracy but slower training (42s)\n",
    "   - NMF: Good accuracy but slowest training (64s)\n",
    "   - KNNBasic: Moderate accuracy but performance issues (had to reduce sample size)\n",
    "   - SlopeOne: Less accurate but extremely fast (0.63s)\n",
    "   - CoClustering: Least accurate but very fast (1.24s)\n",
    "\n",
    "3. **Memory Requirements**:\n",
    "   - KNNBasic failed due to memory constraints\n",
    "   - The other algorithms completed successfully\n",
    "\n",
    "## Recommendation\n",
    "\n",
    "For the MovieLens dataset with 32M ratings, SVD provides the best prediction accuracy. If real-time updates or limited computational resources are a concern, SlopeOne offers a reasonable compromise with significantly faster training times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the results\n",
    "results_df = pd.DataFrame(results).T\n",
    "print(\"\\nResults:\")\n",
    "print(results_df.sort_values('test_rmse'))\n",
    "\n",
    "# Plot the RMSE for each algorithm\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=results_df.index, y='test_rmse', data=results_df)\n",
    "plt.title('RMSE by Algorithm')\n",
    "plt.ylabel('RMSE (lower is better)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
