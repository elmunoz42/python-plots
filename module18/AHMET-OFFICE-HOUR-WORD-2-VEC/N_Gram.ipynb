{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6eUWIszGt-2I"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# N-Gram Models in NLP\n",
        "\n",
        "## ðŸ“– What is an N-Gram?\n",
        "\n",
        "An **N-gram** is a contiguous sequence of $n$ items (usually words or characters) from a given sample of text or speech.\n",
        "\n",
        "- If $n = 1$: **Unigram** (single word)\n",
        "- If $n = 2$: **Bigram** (pair of words)\n",
        "- If $n = 3$: **Trigram** (three-word phrase)\n",
        "- And so on...\n",
        "\n",
        "---\n",
        "\n",
        "## Why Use N-Grams?\n",
        "\n",
        "N-grams are used to capture **local context** and **sequential structure** in text. They are foundational in many NLP tasks such as:\n",
        "\n",
        "- Text classification\n",
        "- Language modeling\n",
        "- Text generation\n",
        "- Spelling correction\n",
        "- Machine translation\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ§¾ Example\n",
        "\n",
        "### Sentence:\n",
        "\n",
        "\"I love natural language processing\"\n",
        "\n",
        "\n",
        "### Unigrams:\n",
        "[\"I\", \"love\", \"natural\", \"language\", \"processing\"]\n",
        "\n",
        "### Bigrams:\n",
        "[\"I love\", \"love natural\", \"natural language\", \"language processing\"]\n",
        "\n",
        "### Trigrams:\n",
        "[\"I love natural\", \"love natural language\", \"natural language processing\"]"
      ],
      "metadata": {
        "id": "Xdl5Q5NTz9N6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "## Mathematical Representation\n",
        "\n",
        "Let $W = w_1, w_2, ..., w_n$ be a sequence of words. The **probability of the entire sequence** is:\n",
        "\n",
        "$$\n",
        "P(w_1, w_2, ..., w_n) = \\prod_{i=1}^{n} P(w_i \\mid w_1, w_2, ..., w_{i-1})\n",
        "$$\n",
        "\n",
        "Using an **N-gram approximation** (Markov assumption):\n",
        "\n",
        "$$\n",
        "P(w_i \\mid w_1, w_2, ..., w_{i-1}) \\approx P(w_i \\mid w_{i-(n-1)}, ..., w_{i-1})\n",
        "$$\n",
        "\n",
        "This simplifies the probability estimation using only the previous $n-1$ words.\n",
        "\n",
        "---\n",
        "\n",
        "## Applications of N-Grams\n",
        "\n",
        "- **Text classification**: Feature engineering with bigrams or trigrams\n",
        "- **Autocomplete**: Predicting next word based on history\n",
        "- **Plagiarism detection**: Matching overlapping n-grams\n",
        "- **Spelling correction**: Detecting unusual n-gram sequences\n",
        "\n",
        "---\n",
        "\n",
        "## âš–ï¸ Pros and Cons\n",
        "\n",
        "| Aspect            | N-Gram Models                         |\n",
        "|-------------------|----------------------------------------|\n",
        "| âœ… Easy to implement   | Yes                                |\n",
        "| âœ… Captures local context | Yes                            |\n",
        "| âŒ Requires large corpus | For high $n$, data sparsity increases |\n",
        "| âŒ No deep semantics     | Context is only partial and local     |\n",
        "\n",
        "---\n",
        "\n",
        "## Summary\n",
        "\n",
        "- N-grams model sequences by capturing **word co-occurrence** patterns.\n",
        "- They are **simple but powerful** tools for understanding context.\n",
        "- Higher-order N-grams can be **more accurate**, but require **more data**.\n",
        "- Often used as **baseline models** or **features** in larger NLP pipelines.\n",
        "\n",
        "> ðŸ’¡ N-gram models were foundational in early NLP, and they still play a role in many practical systems today â€” especially for fast, explainable models.\n"
      ],
      "metadata": {
        "id": "uCfGTLXh0YpB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Sample corpus\n",
        "corpus = [\n",
        "    \"Natural Language Processing is a fascinating field of AI.\",\n",
        "    \"Tokenization breaks text into smaller units called tokens.\",\n",
        "    \"The vocabulary is built from unique words across the corpus.\"\n",
        "]\n",
        "\n",
        "# Loop through 1-gram to 3-gram\n",
        "for n in range(1, 4):\n",
        "    print(f\"\\nðŸ”¢ {n}-Gram Features:\\n\")\n",
        "\n",
        "    # Initialize vectorizer with n-gram range\n",
        "    vectorizer = CountVectorizer(ngram_range=(n, n))\n",
        "    X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "    # Extract and print n-gram features\n",
        "    ngrams = vectorizer.get_feature_names_out()\n",
        "    print(ngrams)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmdAtBOX0c3I",
        "outputId": "f0fe941f-b23b-4ade-ee0f-449f778556e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ”¢ 1-Gram Features:\n",
            "\n",
            "['across' 'ai' 'breaks' 'built' 'called' 'corpus' 'fascinating' 'field'\n",
            " 'from' 'into' 'is' 'language' 'natural' 'of' 'processing' 'smaller'\n",
            " 'text' 'the' 'tokenization' 'tokens' 'unique' 'units' 'vocabulary'\n",
            " 'words']\n",
            "\n",
            "ðŸ”¢ 2-Gram Features:\n",
            "\n",
            "['across the' 'breaks text' 'built from' 'called tokens'\n",
            " 'fascinating field' 'field of' 'from unique' 'into smaller' 'is built'\n",
            " 'is fascinating' 'language processing' 'natural language' 'of ai'\n",
            " 'processing is' 'smaller units' 'text into' 'the corpus' 'the vocabulary'\n",
            " 'tokenization breaks' 'unique words' 'units called' 'vocabulary is'\n",
            " 'words across']\n",
            "\n",
            "ðŸ”¢ 3-Gram Features:\n",
            "\n",
            "['across the corpus' 'breaks text into' 'built from unique'\n",
            " 'fascinating field of' 'field of ai' 'from unique words'\n",
            " 'into smaller units' 'is built from' 'is fascinating field'\n",
            " 'language processing is' 'natural language processing'\n",
            " 'processing is fascinating' 'smaller units called' 'text into smaller'\n",
            " 'the vocabulary is' 'tokenization breaks text' 'unique words across'\n",
            " 'units called tokens' 'vocabulary is built' 'words across the']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QgU110442G99"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}