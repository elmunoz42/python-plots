# -*- coding: utf-8 -*-
"""colab_activity15_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17GIfBlWDot-9XKI4B723EkNgGn6MrfFc

### Colab Activity 15.2: Manual Gradient Descent

**Expected Time = 60 minutes**


This activity focuses on computing the minimum of a function using gradient descent.  Following the examples in the lectures, you will compare the effect of adjusting the step size and the starting point.  The beginning of the work was started in the last activity where one step of size 0.1 was taken.  In this assignment you will complete the work using the update formula provided in the lectures.

#### Index

- [Problem 1](#-Problem-1)
- [Problem 2](#-Problem-2)
- [Problem 3](#-Problem-3)
- [Problem 4](#-Problem-4)
- [Problem 5](#-Problem-5)
"""

import numpy as np
import pandas as pd
import plotly.express as px
import matplotlib.pyplot as plt

"""#### Gradient Descent Formula

Below is the formula for the gradient descent algorithm.  In the formula below:

- $x_{prev}$: Defines the initial x-value
- $df$: Defines the derivative of function interested in optimizing
- $\alpha$: Defines the step size

These values are used to update the value of the previous $x$ as follows:

$$x_{next} = x_{prev} - \alpha * df(x_{prev})$$

Once updated, the $x_{next}$ becomes $x_{prev}$ and the process is repeated until a minimum is found.

[Back to top](#-Index)

### Problem 1

#### Examining the Function

To begin, define a function $f(x) = x^2$ and a domain $x = [-2, 2]$ with 100 points.  


The function is plotted for you to identify whether a maximum or minimum value exists.
"""

def f(x):
  return x**2

x = np.linspace(-2,2,100)


# Answer check
plt.plot(x, f(x), label = 'f')
plt.legend();

"""[Back to top](#-Index)

### Problem 2

#### Taking a step



The function `df` below can be used to approximate the value of a derivative of $f$.


In the code cell below, assign the value of `0.1` to the variable `step_size`. Next, use the formula for gradient descent to compute the next value of $x$ given the starting point `x0`, the function `df`, and `step_size`.

$$x_{1} = x_{0} - \alpha * df(x_{0})$$
"""

def df(x):
  return (f(x + 0.00001) - f(x))/0.00001

x0 = 1
step_size = 0.1
x1 = x0 - step_size * df(x0)



# # Answer check
plt.plot(x0, f(x0), 'o', label = 'Step 0')
plt.plot(x1, f(x1), 'o', label = 'Step 1')
plt.plot(x, f(x))
plt.legend();

"""[Back to top](#-Index)

### Problem 3

#### Repeating the Process



In the code cell below, use a `for` loop with `50` iterations to update the values of $x$ using the gradiet descent formula. Store the values inside the list `xs`. For the derivative, use the `df` function defined previously and use a step size of `0.1`.



$$x_{next} = x_{prev} - \alpha * df(x_{prev})$$
"""

xs = [1]
def simple_gradient_descent(step_size, derivative_function):
  for i in range(50):
    current_x = xs[-1] # get the last one
    xs.append(current_x - step_size * derivative_function(current_x))

simple_gradient_descent(0.1, df)
print(len(xs)) #should be 51

x = np.linspace(-3, 3, 1000)
plt.figure(figsize = (15, 6))
for i, x0 in enumerate(xs):
  plt.text(x0, f(x0), f'Step: {i}');
plt.axvline(x0, color = 'red', alpha = 0.3)
plt.plot(x, f(x), color = 'gray')
plt.plot(xs, f(np.array(xs)), 'o')
plt.xlim(-.3, 1.1)
plt.ylim(-0.1, 1.1)
plt.title('50 Iterations of Gradient Descent')
plt.grid();

"""[Back to top](#-Index)

### Problem 4

#### Adjusting the Step Size



Repeat the above process but this time use `step_size = 0.01`.  Again, compute 50 iterations of gradient descent tracking the updated values in the list `xs`.  What is different about the results this time?  

$$x_{next} = x_{prev} - \alpha * df(x_{prev})$$
"""

xs = [1]
simple_gradient_descent(0.01, df)

print(len(xs)) #should be 51
print([round(i, 2) for i in xs[-5:]])

x = np.linspace(-3, 3, 1000)
plt.figure(figsize = (15, 6))
for i, x0 in enumerate(xs):
  plt.text(x0, f(x0), f'Step: {i}');
  plt.axvline(x0, color = 'red', alpha = 0.3)
plt.plot(x, f(x), color = 'gray', label = 'f')
plt.plot(xs, f(np.array(xs)), 'o')
plt.xlim(-.3, 1.1)
plt.ylim(-0.1, 1.1)
plt.legend()
plt.title('50 Iterations of Gradient Descent')
plt.grid();

"""[Back to top](#-Index)

### Problem 5

#### Adjusting the starting value



Now, you are to examine the effect of changing the starting value. To demonstrate the effect of changing the starting point, consider instead the function

$$f(x) = x^2(x - 2)(x + 1)$$

Use gradient descent to examine the minimum value found with $x_0 = 2$ compared to that when $x_0 = -2$.  Which of the starting values locates the **absolute minimum** of the function $f(x)$?  Assign your solution as an integer to `best_start` below.
"""

def f(x):
  return x**2*(x - 2)*(x + 1)

plt.plot(x, f(x))
plt.title(r'$f(x) = x^2(x - 2)(x + 1)$')
plt.grid();

def df2(x):
  return (f(x + 0.00001) - f(x))/0.00001
xs = [2]
simple_gradient_descent(0.01, df2)

best_start = 2



### ANSWER CHECK
print(f'Minimum is at x = {best_start}')
plt.plot(x, f(x))
plt.plot(xs, f(np.array(xs)), 'o')

plt.plot(x, f(x))
plt.plot(xs, f(np.array(xs)), 'o')

"""Now that you see the importance of both the starting value and step size parameter, consider looking for further refinements to the basic gradient descent model.  In the next activity, you will explore how to use gradient descent in identifying the parameters for a linear regression model.  Here, you will minimize the **MEAN SQUARED ERROR** function rather than an aribitrary polynomial."""