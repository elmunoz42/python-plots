# -*- coding: utf-8 -*-
"""colab_activity12_3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jAz5LNNyXedPC3oZcAQ-qFEwLS8m0HjN

### Colab Activity 12.3: Evaluation Curves â€“ Precision vs. Recall and ROC

**Estimated Time: 90 Minutes**


This assignment focuses on using the precision-recall curves and receiver operating characteristic (ROC) curves to examine tradeoffs in classifier performance.  Also, these curves and the area they determine can be viewed as a metric itself.  Scikit-learn implements all of this, and by the end of this activity, you should be comfortable with these functions.

#### Index

- [Problem 1](#Problem-1)
- [Problem 2](#Problem-2)
- [Problem 3](#Problem-3)
- [Problem 4](#Problem-4)
- [Problem 5](#Problem-5)
- [Problem 6](#Problem-6)
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.neighbors import KNeighborsClassifier
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, recall_score, precision_score, precision_recall_curve, roc_curve
from sklearn.datasets import load_breast_cancer
from sklearn.compose import make_column_transformer
from sklearn import set_config

set_config(display="diagram")

"""### The Data


For this exercise, you will again use the credit card default data.  It is loaded and split below.  A pipeline for model building is also instantiated and fit.
"""

default = pd.read_csv('data/default.csv', index_col=0)

default.head()

transformer = make_column_transformer((OneHotEncoder(drop = 'if_binary'), ['student']),
                                     remainder = StandardScaler())
X_train, X_test, y_train, y_test = train_test_split(default.drop('default', axis = 1), default.default,
                                                   random_state=42,
                                                   stratify = default.default)
knn_pipe = Pipeline([('transform', transformer), ('knn', KNeighborsClassifier(n_neighbors = 10))])

knn_pipe.fit(X_train, y_train)

test_predictions = knn_pipe.predict_proba(X_test)
print(test_predictions)

"""[Back to top](#-Index)

### Problem 1

#### Precision for different thresholds



Below, complete the function `precision_thresh` to take in a NumPy array of predicted probabilities and return a prediction for the positive class at or above that threshold (`thresh`), and a negative prediction for the ones below.  

The function should use the `precision_score` function to return the precision score between `y_test` and `preds` with `pos_label='Yes'`.
"""

def precision_thresh(predict_probs,
                  y_test,
                  thresh,
                  negative='No',
                  positive='Yes'):
    """Calculates precision for a given threshold.

    Given predicted probabilities and a threshold, this function computes
    predictions for the positive class at or above the threshold and returns
    the precision score for those predictions against the test data.

    Parameters
    ----------
    predict_probs : np.ndarray
        1D NumPy array of probabilities for the positive class.
    y_test : np.ndarray
        1D NumPy array of true class labels.
    thresh : float
        Threshold for positive classification (at or above).
    negative : str, optional
        Label for the negative class (default: 'No').
    positive : str, optional
        Label for the positive class (default: 'Yes').

    Returns
    -------
    float
        Precision score for the given threshold.
    """
    true_positives = (predict_probs >= thresh) & (y_test == positive)
    false_positives = (predict_probs >= thresh) & (y_test == negative)

    return true_positives.sum() / (true_positives.sum() + false_positives.sum())

# Answer check
print(precision_thresh(test_predictions[:, 1], y_test, 0.1))
print(precision_thresh(test_predictions[:, 1], y_test, 0.9))

"""[Back to top](#-Index)

### Problem 2

#### Determining precision for multiple thresholds



Now, create a DataFrame called `results_df`. This DataFrame will have a column named `threshold'` with values from the `thresholds` list. The DataFrame will also have a column named `precision`. This column should contain the precision values at each threshold calculated using the `precision_thresh` function.


"""

thresholds = np.arange(0, 1, .1)
thresholds

def calculate_precisions(predict_probs, y_test, thresholds):
    precisions = []
    for thresh in thresholds:
        precisions.append(precision_thresh(predict_probs, y_test, thresh))
    return precisions

results_df = pd.DataFrame({'threshold': thresholds, 'precision': calculate_precisions(test_predictions[:, 1], y_test, thresholds)})



# Answer check
results_df

from matplotlib import pyplot as plt
results_df.plot(kind='scatter', x='threshold', y='precision', s=32, alpha=.8)
plt.gca().spines[['top', 'right',]].set_visible(False)

#Uncomment to visualize precision changes
plt.plot(results_df['threshold'], results_df['precision'], '--o', label = 'precision')
plt.xticks(thresholds)
plt.xlabel('Threshold')
plt.ylabel('Precision')
plt.grid();

"""[Back to top](#-Index)

### Problem 3

#### A Recall threshold function



Similar to your `precision_thresh` function, complete the function `recall_thresh` below to compute the recall given a threshold.  Your function should use the `recall_score` function to return the recall score between `y_test` and `preds` with `pos_label='Yes'`.


Inside the `result_df` DataFrame, add a column called `recall`. This column should contain the recall values at each threshold calculated using the `recall_thresh` function.
"""



def recall_thresh(predict_probs,
                  y_test,
                  thresh,
                  negative='No',
                  positive='Yes'):
    """Given predicted probabilities and a threshold, this function
    computes predictions for the positive class at or above the threshold
    and returns the subesequent recall score for that thresholds predictions
    against the test data.

    Parameters
    ----------
    predict_probs: type 'np.ndarray'
        1D NumPy array of probabilities for positive class
    y_test: type `np.ndarray`
        1D NumPy array of test label
    thresh: type `float`
        threshold for positive classification at or above

    Returns a float for recall value
    """
    true_positives = (predict_probs >= thresh) & (y_test == positive)
    false_negatives = (predict_probs < thresh) & (y_test == positive)

    return true_positives.sum() / (true_positives.sum() + false_negatives.sum())



# Answer check
print(recall_thresh(test_predictions[:, 1], y_test, 0.1))
print(recall_thresh(test_predictions[:, 1], y_test, 0.9))
results_df

"""[Back to top](#-Index)

### Problem 4

#### Precision vs. Recall Tradeoff



As you see in the plot below, based on the `results_df` improving the precision involves a decrease in recall.  

<center>
    <img src = 'images/precall.png'/>
</center>

scikit learn implements a function `precision_recall_curve` that takes as arguments `y_true`, `probas_pred`, `pos_label=None`.

The function returns the values for precision, recall, and the decision thresholds.  Use the probabilities in `test_predictions` in the `precision_recall_curve` function, and assign the results to `precision`, `recall`, and `boundaries` below.  Uncomment the plot to visualize your results.
"""

precision, recall, boundaries = precision_recall_curve(y_test, test_predictions[:, 1], pos_label='Yes')



# Answer check
print(boundaries[:5])
plt.plot(precision, recall, '--o')
plt.xticks(boundaries);
plt.grid()
plt.ylabel('Recall')
plt.xlabel('Precision')
plt.title('Precision vs Recall from sklearn');

"""[Back to top](#-Index)

### Problem 5

#### ROC Curve



Similar to the `precision_recall_curve` the `roc_curve` function takes in `y_true` and `y_score`, which can be predicted probabilities.  The function returns the false positive rates, true positive rates, and thresholds.  Assign these to `fpr`, `tpr`, and `thresh_rocs` below.  Uncomment the code to visualize the ROC curve.
"""

fpr, tpr, thresh_rocs = roc_curve(y_test, test_predictions[:, 1], pos_label='Yes')



# Answer check
print('False Positive Rates: ', fpr[:5])
print('True Positive Rates: ', tpr[:5])
plt.plot(fpr, tpr, '--o', label = 'roc curve')
plt.plot(tpr, tpr, label = 'baseline')
plt.legend()
plt.title('ROC Curve')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.grid();

"""[Back to top](#-Index)

### Problem 6

#### Area Under Curve as metric



Finally, consider the proposed area under the curve scoring method described in the lectures.  Below, construct a grid search named `roc_grid` that uses the `knn_pipe` and searches over the number of neighbors from 1 to 31 by odd values, choosing the model that optimizes `roc_auc_score`.  Identify the optimal number of neighbors and assign them to `best_k` below.  
"""

roc_grid = GridSearchCV(knn_pipe, {'knn__n_neighbors': range(1, 32, 2)}, scoring = 'roc_auc')
best_k = roc_grid.fit(X_train, y_train).best_params_['knn__n_neighbors']


# Answer check
best_k

"""#### Visualizing with scikitlearn

Below, we use the `RocCurveDisplay` to display roc curves for three knn models.  You can either use predictions or estimators to create the visualization.  Below, we use the `from_estimator` method.
"""

from sklearn.metrics import RocCurveDisplay

knn_1 = Pipeline([('transform', transformer), ('knn', KNeighborsClassifier(n_neighbors = 1))])
knn_1.fit(X_train, y_train)

fig, ax = plt.subplots()
RocCurveDisplay.from_estimator(roc_grid, X_test, y_test, pos_label = 'Yes', ax = ax, label = 'Grid Search: 29 Neighbors')
RocCurveDisplay.from_estimator(knn_pipe, X_test, y_test, pos_label = 'Yes', ax = ax, label = '10 Neighbors')
RocCurveDisplay.from_estimator(knn_1, X_test, y_test, ax = ax, label = '1 Neighbor')
plt.grid()
plt.plot(np.arange(0, 1.1, .1), np.arange(0, 1.1, .1), label = 'baseline');
plt.title('Using RocCurveDisplay')
plt.legend();



