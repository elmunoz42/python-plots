{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "636723efd4901b487dca9d0cae5aa010",
     "grade": false,
     "grade_id": "cell-9ac57c22dea04d1d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Codio Activity 12.6: Evaluation Curves: Precision vs. Recall and ROC\n",
    "\n",
    "**Estimated Time: 90 Minutes**\n",
    "\n",
    "**Total Points: 60**\n",
    "\n",
    "This assignment focuses on using the precision recall curves and receiver operating characteristic (ROC) curves to examine tradeoffs in classifier performance.  Also, these curves and the area determined by them can be viewed as a metric itself.  Scikit-learn implements all of this, and by the end of this activity you should be comfortable with these functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cfb4d436bcd62c230b378541d7b9d885",
     "grade": false,
     "grade_id": "cell-ede1a8b81a520136",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "#### Index\n",
    "\n",
    "- [Problem 1](#Problem-1)\n",
    "- [Problem 2](#Problem-2)\n",
    "- [Problem 3](#Problem-3)\n",
    "- [Problem 4](#Problem-4)\n",
    "- [Problem 5](#Problem-5)\n",
    "- [Problem 6](#Problem-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, precision_recall_curve, roc_curve\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn import set_config\n",
    "\n",
    "set_config(display=\"diagram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "000de869b477bd02e15866514d6e85c3",
     "grade": false,
     "grade_id": "cell-7c6f528bbe21a8ac",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### The Data\n",
    "\n",
    "\n",
    "For this exercise, you will again use the credit card default data.  It is loaded and split below.  A pipeline for model building is also instantiated and fit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default = pd.read_csv('data/default.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = make_column_transformer((OneHotEncoder(drop = 'if_binary'), ['student']),\n",
    "                                     remainder = StandardScaler())\n",
    "X_train, X_test, y_train, y_test = train_test_split(default.drop('default', axis = 1), default.default,\n",
    "                                                   random_state=42,\n",
    "                                                   stratify = default.default)\n",
    "knn_pipe = Pipeline([('transform', transformer), ('knn', KNeighborsClassifier(n_neighbors = 10))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = knn_pipe.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b1db936660d2fcb7fbeecbdcd4910b7e",
     "grade": false,
     "grade_id": "cell-9c1f15d5341ac144",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#-Index)\n",
    "\n",
    "### Problem 1\n",
    "\n",
    "#### Precision for different thresholds\n",
    "\n",
    "**10 Points**\n",
    "\n",
    "Below, complete the function `precision_thresh` to take in a NumPy array of predicted probabilities, and return a prediction for the positive class at or above that threshold (`thresh`), and negative prediction for the ones below.  \n",
    "\n",
    "The function should use the `precision_score` function to return the precision score between `y_test` and `preds` with `pos_label='Yes'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c8b70f07689aba845320d88efa1dd395",
     "grade": false,
     "grade_id": "cell-aa2f28ba58aff633",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "def precision_thresh(predict_probs, \n",
    "                  y_test, \n",
    "                  thresh):\n",
    "    \"\"\"Given predicted probabilities and a threshold, this function\n",
    "    computes predictions for the positive class at or above the threshold\n",
    "    and returns the subesequent precision score for that thresholds predictions \n",
    "    against the test data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    predict_probs: type 'np.ndarray'\n",
    "        1D NumPy array of probabilities for positive class\n",
    "    y_test: type `np.ndarray`\n",
    "        1D NumPy array of test label\n",
    "    thresh: type `float`\n",
    "        threshold for positive classification at or above\n",
    "          \n",
    "    Returns a float for precision value\n",
    "    \"\"\"\n",
    "    return None\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Answer check\n",
    "print(precision_thresh(test_predictions[:, 1], y_test, 0.1))\n",
    "print(precision_thresh(test_predictions[:, 1], y_test, 0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4a41521100d03b971f33dd0c97016c33",
     "grade": true,
     "grade_id": "cell-17e6c60c93406da9",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1044686dd1554a583c84ca2c2dc68496",
     "grade": false,
     "grade_id": "cell-c7c8229811bcd199",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#-Index)\n",
    "\n",
    "### Problem 2\n",
    "\n",
    "#### Determining precision for multiple thresholds\n",
    "\n",
    "**10 Points**\n",
    "\n",
    "Now, create a DataFrame called `results_df`. This DataFrame will have a column named `threshold'` with values from the `thresholds` list. The DataFrame will also have a column named `precision`. This column should contain the precision values at each threshold calculated using the `precision_thresh` function. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = np.arange(0, 1, .1)\n",
    "thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d5702246d368f90dfb538c97b81c0084",
     "grade": false,
     "grade_id": "cell-1f9418ad30a6cb2a",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "results_df = ''\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Answer check\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "24e622d7b37a5f18adc5afaffee1095e",
     "grade": true,
     "grade_id": "cell-fb4854d718e9a8f2",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Uncomment to visualize precision changes\n",
    "# plt.plot(precision_df['threshold'], precision_df['precision'], '--o', label = 'precision')\n",
    "# plt.xticks(thresholds)\n",
    "# plt.xlabel('Threshold')\n",
    "# plt.ylabel('Precision')\n",
    "# plt.grid();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b716493fd183dc48c5f1ee3531ba91d8",
     "grade": false,
     "grade_id": "cell-eb82f4a98d283216",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#-Index)\n",
    "\n",
    "### Problem 3\n",
    "\n",
    "#### A Recall threshold function\n",
    "\n",
    "**10 Points**\n",
    "\n",
    "Similar to your `precision_thresh` function, complete the function `recall_thresh` below to compute the recall given a threshold.  Your function should use the `recall_score` function to return the recall score between `y_test` and `preds` with `pos_label='Yes'`.\n",
    "\n",
    "\n",
    "Inside the `result_df` DataFrame, add a column called `recall`. This column should contain the recall values at each threshold calculated using the `recall_thresh` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b9fda85c604b929c16d5c14e3b3bdea6",
     "grade": false,
     "grade_id": "cell-bf0288d8c9fa0106",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "def recall_thresh(predict_probs, \n",
    "                  y_test, \n",
    "                  thresh):\n",
    "    \"\"\"Given predicted probabilities and a threshold, this function\n",
    "    computes predictions for the positive class at or above the threshold\n",
    "    and returns the subesequent recall score for that thresholds predictions \n",
    "    against the test data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    predict_probs: type 'np.ndarray'\n",
    "        1D NumPy array of probabilities for positive class\n",
    "    y_test: type `np.ndarray`\n",
    "        1D NumPy array of test label\n",
    "    thresh: type `float`\n",
    "        threshold for positive classification at or above\n",
    "          \n",
    "    Returns a float for recall value\n",
    "    \"\"\"\n",
    "    return None\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Answer check\n",
    "print(recall_thresh(test_predictions[:, 1], y_test, 0.1))\n",
    "print(recall_thresh(test_predictions[:, 1], y_test, 0.9))\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2fec39595c861f38647042fa8dcc6cc6",
     "grade": true,
     "grade_id": "cell-c7b044a5e8a5bef0",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "de29adeca7dc74b8f7a9b97f407a8612",
     "grade": false,
     "grade_id": "cell-fdcbbf10b3a88130",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#-Index)\n",
    "\n",
    "### Problem 4\n",
    "\n",
    "#### Precision vs. Recall Tradeoff\n",
    "\n",
    "**10 Points**\n",
    "\n",
    "As you see in the plot below based on the `results_df` improving the precision involves a decrease in recall.  \n",
    "\n",
    "<center>\n",
    "    <img src = 'images/precall.png'/>\n",
    "</center>\n",
    "\n",
    "scikit learn implements a function `precision_recall_curve` that takes as arguments `y_true`, `probas_pred`, `pos_label=None`. \n",
    "\n",
    "The function returns the values for precision, recall, and the decision thresholds.  Use the probabilities in `test_predictions` in the `precision_recall_curve` function, and assign the results to `precision`, `recall`, and `boundaries` below.  Uncomment the plot to visualize your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "95d13f5ec596d8707fb61cb40694b549",
     "grade": false,
     "grade_id": "cell-5f1475c664f2084a",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "precision, recall, boundaries = '', '', ''\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Answer check\n",
    "print(boundaries[:5])\n",
    "# plt.plot(precision, recall, '--o')\n",
    "# plt.xticks(boundaries);\n",
    "# plt.grid()\n",
    "# plt.ylabel('Recall')\n",
    "# plt.xlabel('Precision')\n",
    "# plt.title('Precision vs Recall from sklearn');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "885ae9f2e2337f75ffb08093616a1e7f",
     "grade": true,
     "grade_id": "cell-7cb71b2cc1d2594c",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "21fd93e873142bc5f3bad2f599332d80",
     "grade": false,
     "grade_id": "cell-c695fe027b8abd97",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#-Index)\n",
    "\n",
    "### Problem 5\n",
    "\n",
    "#### ROC Curve\n",
    "\n",
    "**10 Points**\n",
    "\n",
    "Similar to the `precision_recall_curve` the `roc_curve` function takes in `y_true` and `y_score` which can be predicted probabilities.  The function returns the false positive rates, true positive rates, and thresholds.  Assign these to `fpr`, `tpr`, and `thresh_rocs` below.  Uncomment the code to visualize the ROC curve. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d70307d4a811879488cb3550a90f3c0d",
     "grade": false,
     "grade_id": "cell-351b4ccfb60cccf7",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "fpr, tpr, thresh_rocs = '', '', ''\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Answer check\n",
    "print('False Positive Rates: ', fpr[:5])\n",
    "print('True Positive Rates: ', tpr[:5])\n",
    "# plt.plot(fpr, tpr, '--o', label = 'roc curve')\n",
    "# plt.plot(tpr, tpr, label = 'baseline')\n",
    "# plt.legend()\n",
    "# plt.title('ROC Curve')\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.grid();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cd08ad8a6d33b290cd6f173512011fba",
     "grade": true,
     "grade_id": "cell-0f22a7829165a75e",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d3439fb19467f86d434baec8ea57a728",
     "grade": false,
     "grade_id": "cell-0d0f0fd4693f1894",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#-Index)\n",
    "\n",
    "### Problem 6\n",
    "\n",
    "#### Area Under Curve as metric\n",
    "\n",
    "**10 Points**\n",
    "\n",
    "Finally, consider the proposed area under the curve scoring method described in the lectures.  Below, construct a grid search named `roc_grid` that uses the `knn_pipe` and searches over the number of neighbors from 1 to 31 by odd values, choosing the model that optimizes `roc_auc_score`.  Identify the optimal number of neighbors and assign to `best_k` below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ac383287506b4811584dad5a8f9a53a5",
     "grade": false,
     "grade_id": "cell-757458e9af6400ec",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "roc_grid = ''\n",
    "best_k = ''\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Answer check\n",
    "best_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ada1b44b8ce536d687cf98b6f4c2fed9",
     "grade": true,
     "grade_id": "cell-dcd5ef8c57953964",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "96ef2ced738e46b349057cae0cd04d0f",
     "grade": false,
     "grade_id": "cell-bd022abdcd502403",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "#### Visualizing with scikitlearn\n",
    "\n",
    "Below, we use the `RocCurveDisplay` to display roc curves for three knn models.  You can either use predictions or estimators to create the visualization.  Below, we use the `from_estimator` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import RocCurveDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_1 = Pipeline([('transform', transformer), ('knn', KNeighborsClassifier(n_neighbors = 1))])\n",
    "knn_1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "RocCurveDisplay.from_estimator(roc_grid, X_test, y_test, pos_label = 'Yes', ax = ax, label = 'Grid Search: 29 Neighbors')\n",
    "RocCurveDisplay.from_estimator(knn_pipe, X_test, y_test, pos_label = 'Yes', ax = ax, label = '10 Neighbors')\n",
    "RocCurveDisplay.from_estimator(knn_1, X_test, y_test, ax = ax, label = '1 Neighbor')\n",
    "plt.grid()\n",
    "plt.plot(np.arange(0, 1.1, .1), np.arange(0, 1.1, .1), label = 'baseline');\n",
    "plt.title('Using RocCurveDisplay')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
